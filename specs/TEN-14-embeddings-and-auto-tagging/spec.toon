plan:
  title: ten-14-local-embeddings-semantic-search-and-byok-auto-tagging
  goal: "Add a minimal local-first embeddings pipeline that persists vectors in the local sqlite cache via sqlite-vec, powers semantic document search, and uses BYOK OpenAI calls for low-cost auto-tagging based on semantic neighbors."
  steps[6]:
    - title: extend-local-cache-schema-with-embeddings-and-semantic-search-commands
      goal: "Add sqlite-vec virtual table and Tauri invoke commands to store document embeddings and run KNN similarity search locally without hand-rolled cosine math."
      context: "Local cache storage is implemented in `core/src/document_cache.rs` and command wiring is in `frontend/src-tauri/src/lib.rs`. Existing cache DB file is `<documents_folder>/.document-data.db` with `documents` and `document_tags` tables. Existing command style is `#[tauri::command] fn name(args) -> Result<Payload, String>` and commands are registered in `tauri::generate_handler!`. The existing payload naming convention uses snake_case fields matching frontend `DocumentListItem` and tag usage payloads. `core/src/lib.rs` already exports `document_cache`. The `sqlite-vec` crate (`0.0.1-alpha.4`) provides a SQLite extension that adds a `vec0` virtual table type for KNN search; it is loaded at runtime via `sqlite_vec::sqlite3_auto_extension` and requires no dynamic library bundling."
      instructions: "Modify `core/Cargo.toml`, `core/src/document_cache.rs`, and `frontend/src-tauri/src/lib.rs` in this step. In `core/Cargo.toml`, add dependency `sqlite-vec = '0.0.1-alpha.4'`. In `core/src/document_cache.rs`, call `unsafe { sqlite_vec::sqlite3_auto_extension(Some(sqlite_vec::sqlite3_vec_init)); }` once before any connection is opened (guard with a `std::sync::Once`). Extend schema creation SQL with two objects: a regular table `document_embeddings_meta` containing `id INTEGER PRIMARY KEY AUTOINCREMENT`, `document_id TEXT NOT NULL UNIQUE` (foreign key to `documents(id)` with `ON DELETE CASCADE`), `model TEXT NOT NULL`, `content_hash TEXT NOT NULL`, and `updated_at TEXT NOT NULL`; add indexes on `updated_at`, `content_hash`, and `document_id`. Then create a virtual table `document_embeddings_vec USING vec0(embedding 384-dimensional float vector)` whose `rowid` is kept in sync with `document_embeddings_meta.id`. Add serde payload structs `CachedDocumentEmbeddingPayload { document_id: String, model: String, content_hash: String, vector: Vec<f32>, updated_at: String }`, `CachedDocumentEmbeddingMetadataPayload { document_id: String, model: String, content_hash: String, updated_at: String }`, and `SemanticSearchHitPayload { document_id: String, score: f32 }`. Add store methods: upsert one embedding (insert or replace in both tables atomically, storing vector bytes via `vec_f32(?)`); delete one embedding (delete from meta, cascade removes vec row); list embedding metadata; replace all embeddings transactionally (delete all meta rows then bulk insert). For semantic search, accept `query_vector: Vec<f32>`, `limit: usize`, `min_score: f32`, and `optional exclude_document_id: Option<String>`. Because `embedTextLocally` always returns unit-normalized vectors, sqlite-vec L2 distance satisfies `cosine_similarity = 1.0 - (distance * distance / 2.0)`; convert `min_score` to `max_distance = (2.0 * (1.0 - min_score)).sqrt()` and use it as a post-filter. Execute KNN query `SELECT v.rowid, v.distance FROM document_embeddings_vec v WHERE v.embedding MATCH vec_f32(?) AND k = ?` (pass limit * 4 as k to allow for post-filtering), join to meta on rowid, exclude by document_id if provided, apply max_distance filter, compute score, sort descending by score with stable tie-break by `document_id`, and return at most `limit` results. Validate non-empty vectors (length must equal 384) before insert or search and return an error string otherwise. In `frontend/src-tauri/src/lib.rs`, add commands `get_cached_document_embedding_metadata(documents_folder: String)`, `upsert_cached_document_embedding(documents_folder: String, embedding: CachedDocumentEmbeddingPayload)`, `delete_cached_document_embedding(documents_folder: String, document_id: String)`, `replace_cached_document_embeddings(documents_folder: String, embeddings: Vec<CachedDocumentEmbeddingPayload>)`, and `semantic_search_cached_documents(documents_folder: String, query_vector: Vec<f32>, limit: usize, min_score: f32, exclude_document_id: Option<String>)`; register all new commands without changing existing ones."
      verification: "Run `cargo check -p tentacle-core && cargo check -p app` from repo root and confirm the new commands/types compile with the sqlite-vec dependency."
    - title: add-frontend-local-embedding-model-service-and-embedding-command-wrapper
      goal: "Introduce a reusable local embedding generator with download progress feedback and typed frontend wrappers for the new embedding/cache commands."
      context: "Frontend invoke wrappers follow the pattern in `frontend/lib/documents/cache.ts`, including payload normalization, dual camelCase/snake_case invoke args, and safe fallbacks. OpenAI BYOK settings helpers already exist in `frontend/lib/settings/openai-config.ts`. Document API contracts are in `frontend/types/documents.ts`. The new Tauri commands available are `get_cached_document_embedding_metadata`, `upsert_cached_document_embedding`, `delete_cached_document_embedding`, `replace_cached_document_embeddings`, and `semantic_search_cached_documents`. Vectors cross the Tauri bridge as `number[]` / `Vec<f32>`; the sqlite-vec storage detail is transparent to the frontend layer. `@xenova/transformers` downloads model weights (~80MB) on first use; without feedback this looks like a freeze, so a toast notification must be shown during the download."
      instructions: "Modify `frontend/package.json` to add two dependencies: `@xenova/transformers` and `react-hot-toast`. Ensure `<Toaster />` from `react-hot-toast` is mounted once in the app layout (add it to `frontend/app/layout.tsx` or the nearest persistent shell component if a `<Toaster />` is not already present). Create `frontend/lib/ai/local-embeddings.ts` exporting `LOCAL_EMBEDDING_MODEL_ID = 'Xenova/all-MiniLM-L6-v2'`, `embedTextLocally(text: string): Promise<number[]>`, and `extractPlainTextFromTiptapBody(body: string): string`. Implement lazy singleton loading of the feature-extraction pipeline. On the first load attempt (pipeline not yet initialized), fire a persistent loading toast with id `'model-download'` and message 'Downloading AI model (first time onlyâ€¦)' using `toast.loading`. Pass a `progress_callback` to the pipeline constructor; when the callback receives `status === 'ready'`, dismiss the loading toast and show a brief success toast 'AI model ready'. On subsequent calls the singleton is already initialized so no toast is shown. Return a normalized float vector for non-empty text; return empty array for blank input. Create `frontend/lib/documents/embeddings-cache.ts` with typed wrappers `readCachedEmbeddingMetadata(folder: string): Promise<DocumentEmbeddingMetadata[]>`, `upsertCachedDocumentEmbedding(folder: string, embedding: CachedDocumentEmbeddingPayload): Promise<void>`, `deleteCachedDocumentEmbedding(folder: string, documentId: string): Promise<void>`, `replaceCachedDocumentEmbeddings(folder: string, embeddings: CachedDocumentEmbeddingPayload[]): Promise<void>`, and `semanticSearchCachedDocuments(folder: string, args): Promise<SemanticSearchHit[]>`. Update `frontend/types/documents.ts` with interfaces `DocumentEmbeddingMetadata`, `CachedDocumentEmbeddingPayload`, and `SemanticSearchHit` using snake_case fields (`document_id`, `content_hash`, `updated_at`). Keep all wrappers tolerant to malformed payloads and sorted deterministically."
      verification: "Run `cd frontend && npm install && npx tsc --noEmit && npm run lint` and confirm type/lint checks pass with the new modules and dependencies. Manually trigger the first load and confirm the loading toast appears and resolves."
    - title: wire-embedding-lifecycle-into-document-create-update-delete-and-reindex
      goal: "Ensure embeddings are created and refreshed automatically after local document writes, while keeping CRUD latency fast."
      context: "Document CRUD and reindex logic is in `frontend/lib/documents/api.ts` with exports `createDocument`, `updateDocument`, `deleteDocument`, `reindexDocuments`, `fetchCachedDocuments`, and `fetchCachedDocumentTags`. Local sqlite cache sync already happens there via `upsertCachedDocument`, `deleteCachedDocument`, and `replaceCachedDocuments`. New embedding helpers now exist in `frontend/lib/ai/local-embeddings.ts` and `frontend/lib/documents/embeddings-cache.ts`, and `getDocumentsFolderAsync()` resolves the active folder path."
      instructions: "Modify only `frontend/lib/documents/api.ts` in this step. Add private helpers to build embedding source text from `title + plain text body`, compute a deterministic `content_hash` (SHA-256 hex from source text + model id), and upsert embeddings in the local cache. After successful `createDocument` and `updateDocument` writes, trigger non-blocking embedding sync (`void` async task) that skips recomputation when metadata already has matching `content_hash`. After `deleteDocument`, delete the embedding row for that document. In `reindexDocuments`, after `replaceCachedDocuments`, schedule a batched embedding sync for all indexed documents with small concurrency (for example 2 at a time) and error logging without throwing. Keep existing CRUD return values and error messages unchanged; embedding failures must never block document save/delete paths."
      verification: "Run `cd frontend && npx tsc --noEmit && npm run lint` and verify by code inspection that `createDocument`, `updateDocument`, `deleteDocument`, and `reindexDocuments` each call the new embedding sync/delete helpers."
    - title: add-semantic-search-api-and-dashboard-search-experience
      goal: "Expose semantic search from the document API and add a simple dashboard search box that ranks documents by embedding similarity."
      context: "Dashboard shell is `frontend/app/app/page.tsx` and document list logic is `frontend/components/documents/document-grid.tsx`. `DocumentGrid` already handles cached-first loading, background reindex, tag filtering, and stale-request protection via `requestIdRef`. Debounce utility exists in `frontend/hooks/use-debounce.ts`. Embeddings are now persisted in sqlite-vec and queryable through `semanticSearchCachedDocuments(...)` from `frontend/lib/documents/embeddings-cache.ts`."
      instructions: "Modify `frontend/lib/documents/api.ts`, `frontend/components/documents/document-grid.tsx`, and `frontend/app/app/page.tsx`. In `frontend/lib/documents/api.ts`, export `semanticSearchDocuments(query: string, options?: { limit?: number; min_score?: number; exclude_document_id?: string }): Promise<SemanticSearchHit[]>` that embeds the query locally and calls `semanticSearchCachedDocuments`; return empty list for blank query or missing vector. In `frontend/app/app/page.tsx`, add a compact search input above `<DocumentGrid />` and debounce the value. Change `DocumentGrid` to accept prop `searchQuery: string`. In `frontend/components/documents/document-grid.tsx`, when `searchQuery` is non-empty run semantic search, map hit order onto cached `documents`, and then apply existing selected-tag filtering; keep `NewDocumentCard` visible regardless of search state. Add stale-request guards so older search responses cannot overwrite newer results. Add a lightweight fallback: if semantic search fails, filter documents by case-insensitive substring match on title/body preview. Keep existing synchronizing and tag filter behavior intact."
      verification: "Run `cd frontend && npx tsc --noEmit && npm run lint && npm run build`, then run `cd frontend && npm run dev` and confirm non-empty search text changes ordering/results semantically while tag filters still work."
    - title: implement-byok-auto-tagging-powered-by-semantic-neighbor-context
      goal: "Add a minimal auto-tagging pipeline that uses cheap OpenAI BYOK calls and semantic-neighbor tags for consistent suggestions."
      context: "Document detail autosave flow is in `frontend/app/app/documents/page.tsx` and currently saves title, body, and tags via `updateDocument`. OpenAI key access helper is `getOpenAIApiKey()` in `frontend/lib/settings/openai-config.ts`; CSP already allows `https://api.openai.com` in `frontend/src-tauri/tauri.conf.json`. Semantic search is available from `semanticSearchDocuments` and document cache list can be read with `fetchCachedDocuments()`. Tag normalization helpers already exist in `frontend/app/app/documents/page.tsx` and should be reused to keep lowercase underscore format."
      instructions: "Create `frontend/lib/ai/auto-tagging.ts` exporting `suggestTagsWithOpenAI(input): Promise<string[]>`. Use `getOpenAIApiKey()` and call `https://api.openai.com/v1/chat/completions` with model `gpt-4o-mini` and low token budget; prompt the model to return JSON array only (max 5 tags, lowercase, underscores, no leading #). Parse defensively and normalize tags before returning. Modify `frontend/app/app/documents/page.tsx` to add background auto-tagging triggered after debounced title/body changes: compute a fingerprint of current title/body, skip duplicate runs, skip very short documents, and skip when API key is missing. For each run, fetch top semantic neighbors (`semanticSearchDocuments`) excluding current doc, gather their tags as candidate vocabulary, send note text + candidate tags to `suggestTagsWithOpenAI`, and merge suggestions into existing tags without removing user tags. Persist merged tags through `updateDocument(doc.id, { tags: mergedTags })` only when changed. Add `isAutoTagging` UI state next to existing `Saving...` status. Update helper text in `frontend/components/settings/settings-modal.tsx` so the OpenAI key description mentions both voice transcription and auto-tagging."
      verification: "Run `cd frontend && npx tsc --noEmit && npm run lint`; then with a configured API key, edit a document and confirm tags are suggested automatically after save, while without a key no auto-tagging request is made."
    - title: validate-end-to-end-behavior-and-document-the-local-ai-flow
      goal: "Confirm embeddings persistence, semantic ranking, and auto-tagging behavior in the desktop app and record the MVP behavior in docs."
      context: "Expected changed files include `core/Cargo.toml`, `core/src/document_cache.rs`, `frontend/src-tauri/src/lib.rs`, `frontend/package.json`, `frontend/types/documents.ts`, `frontend/lib/ai/local-embeddings.ts`, `frontend/lib/ai/auto-tagging.ts`, `frontend/lib/documents/embeddings-cache.ts`, `frontend/lib/documents/api.ts`, `frontend/components/documents/document-grid.tsx`, `frontend/app/app/page.tsx`, `frontend/app/app/documents/page.tsx`, and `frontend/components/settings/settings-modal.tsx`. Local cache DB location is `<documents_folder>/.document-data.db`."
      instructions: "Run full quality checks and a manual smoke test. Commands: `cargo check -p tentacle-core && cargo check -p app`, then `cd frontend && npx tsc --noEmit && npm run lint && npm run build`. Manual smoke flow in Tauri runtime: set documents folder, create or edit multiple docs, wait for background indexing, run semantic queries from dashboard search, and verify results prioritize semantically related notes over strict keyword matches. With API key configured, edit a note and confirm auto-tags appear; verify manual tags are preserved. Inspect sqlite cache with `sqlite3 '<documents_folder>/.document-data.db' '.tables'` and confirm both `document_embeddings_meta` and `document_embeddings_vec` are present; run `sqlite3 '<documents_folder>/.document-data.db' 'SELECT COUNT(*) FROM document_embeddings_meta;'` to confirm embedding row count. Update `README.md` features section with one concise bullet for local embeddings semantic search and one bullet for optional BYOK auto-tagging."
      verification: "All checks pass and manual verification confirms embedding rows exist, semantic search returns ranked hits, and BYOK auto-tagging enriches tags without blocking saves."
  acceptance_criteria[10]:
    - title: embeddings-schema-and-commands-exist
      requirement: "`core/src/document_cache.rs` defines `document_embeddings_meta` and `document_embeddings_vec` (sqlite-vec virtual table) plus embedding/search payload structs and methods, and `frontend/src-tauri/src/lib.rs` registers all embedding/search commands; verify with `cargo check -p tentacle-core && cargo check -p app`."
    - title: local-embedding-service-uses-small-model
      requirement: "`frontend/lib/ai/local-embeddings.ts` loads `Xenova/all-MiniLM-L6-v2` and exports `embedTextLocally` returning a unit-normalized float vector; verify by code inspection and `cd frontend && npx tsc --noEmit`."
    - title: embedding-sync-hooks-are-wired-into-document-lifecycle
      requirement: "`frontend/lib/documents/api.ts` triggers embedding upsert after create/update/reindex and embedding delete after delete, with non-blocking error handling; verify by code inspection."
    - title: embeddings-persist-in-local-sqlite
      requirement: "After creating/updating documents, `<documents_folder>/.document-data.db` contains rows in `document_embeddings_meta` and corresponding rows in `document_embeddings_vec`; verify with `sqlite3 '<documents_folder>/.document-data.db' 'SELECT COUNT(*) FROM document_embeddings_meta;'`."
    - title: semantic-search-api-returns-ranked-hits
      requirement: "`semanticSearchDocuments()` in `frontend/lib/documents/api.ts` returns `SemanticSearchHit[]` sorted by descending score and supports `limit`, `min_score`, and optional exclusion; verify by code inspection and runtime logging/manual checks."
    - title: dashboard-search-uses-semantic-ranking
      requirement: "`frontend/app/app/page.tsx` passes a debounced query to `frontend/components/documents/document-grid.tsx`, and `DocumentGrid` uses semantic hit order when query is non-empty while retaining tag filters and `NewDocumentCard`; verify in runtime UI."
    - title: auto-tagging-uses-byok-openai-and-semantic-candidates
      requirement: "`frontend/lib/ai/auto-tagging.ts` calls OpenAI with `gpt-4o-mini`, and `frontend/app/app/documents/page.tsx` supplies candidate tags derived from semantic neighbors before merging suggestions; verify by code inspection and runtime behavior with API key set."
    - title: auto-tagging-is-safe-and-non-destructive
      requirement: "Auto-tagging never removes existing user tags and only persists when merged tags actually change; verify by editing a document with manual tags and observing saved result."
    - title: save-flow-remains-responsive
      requirement: "Document create/update/delete operations complete even if embedding generation or auto-tagging fails, with errors logged but no blocking exception surfaced to users; verify by forcing failures (missing API key/network) and confirming saves still succeed."
    - title: project-quality-gates-pass
      requirement: "`cargo check -p tentacle-core && cargo check -p app && cd frontend && npx tsc --noEmit && npm run lint && npm run build` runs without errors after implementation."
